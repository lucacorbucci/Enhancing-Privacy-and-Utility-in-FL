{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "Runs MNIST training with differential privacy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from opacus import PrivacyEngine\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "import os\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import Tensor, nn\n",
    "\n",
    "\n",
    "class CelebaNet(nn.Module):\n",
    "    \"\"\"This class defines the CelebaNet.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        num_classes: int = 4,\n",
    "        dropout_rate: float = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the CelebaNet network.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            in_channels (int, optional): Number of input channels . Defaults to 3.\n",
    "            num_classes (int, optional): Number of classes . Defaults to 2.\n",
    "            dropout_rate (float, optional): _description_. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            8,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            stride=(1, 1),\n",
    "        )\n",
    "        self.cnn2 = nn.Conv2d(8, 16, kernel_size=(3, 3), padding=(1, 1), stride=(1, 1))\n",
    "        self.cnn3 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=(1, 1), stride=(1, 1))\n",
    "        self.fc1 = nn.Linear(2048, num_classes)\n",
    "        self.gn_relu = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data: Tensor) -> Tensor:\n",
    "        \"\"\"Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            input_data (Tensor): Input data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor: Output data\n",
    "        \"\"\"\n",
    "        out = self.gn_relu(self.cnn1(input_data))\n",
    "        out = self.gn_relu(self.cnn2(out))\n",
    "        out = self.gn_relu(self.cnn3(out))\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaDataset(Dataset):\n",
    "    \"\"\"Definition of the dataset used for the Celeba Dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        image_path: str,\n",
    "        transform: torchvision.transforms = None,\n",
    "        debug: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the dataset.\n",
    "\n",
    "        Args:\n",
    "        ----\n",
    "            csv_path (str): path of the csv file with all the information\n",
    "             about the dataset\n",
    "            image_path (str): path of the images\n",
    "            transform (torchvision.transforms, optional): Transformation to apply\n",
    "            to the images. Defaults to None.\n",
    "        \"\"\"\n",
    "        dataframe = pd.read_csv(csv_path)\n",
    "\n",
    "        self.targets = dataframe[\"Target\"].tolist()\n",
    "        self.classes = dataframe[\"Target\"].tolist()\n",
    "\n",
    "        self.samples = list(dataframe[\"image_id\"])\n",
    "        self.n_samples = len(dataframe)\n",
    "        self.transform = transform\n",
    "        self.image_path = image_path\n",
    "        self.debug = debug\n",
    "        if not self.debug:\n",
    "            self.images = [\n",
    "                Image.open(os.path.join(self.image_path, sample)).convert(\n",
    "                    \"RGB\",\n",
    "                )\n",
    "                for sample in self.samples\n",
    "            ]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"Returns a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (_type_): index of the sample we want to retrieve\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            _type_: sample we want to retrieve\n",
    "\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            img = Image.open(\n",
    "                os.path.join(self.image_path, self.samples[index]),\n",
    "            ).convert(\n",
    "                \"RGB\",\n",
    "            )\n",
    "        else:\n",
    "            img = self.images[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (\n",
    "            img,\n",
    "            self.targets[index],\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"This function returns the size of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            int: size of the dataset\n",
    "        \"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ],\n",
    ")\n",
    "train_dataset = CelebaDataset(\n",
    "    csv_path=\"../data/celeba/train_smiling.csv\",\n",
    "    image_path=\"../data/celeba/img_align_celeba\",\n",
    "    transform=transform,\n",
    "    debug=True,\n",
    ")\n",
    "test_dataset = CelebaDataset(\n",
    "    csv_path=\"../data/celeba/test_smiling.csv\",\n",
    "    image_path=\"../data/celeba/img_align_celeba\",\n",
    "    transform=transform,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "# train_dataset = torch.load(\"../data/celeba/cluster_0_node_0_private_train.pt\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=512,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, device, privacy_engine):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    DELTA = 1e-5\n",
    "    losses = []\n",
    "    top1_acc = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_loader, max_physical_batch_size=128, optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for i, (images, target) in enumerate(memory_safe_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 50 == 0:\n",
    "                epsilon = privacy_engine.get_epsilon(DELTA)\n",
    "                print(\n",
    "                    f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                    f\"Loss: {np.mean(losses):.6f} \"\n",
    "                    f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                    f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcorbucci/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradSampleModule(CelebaNet(\n",
       "  (cnn1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cnn2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (cnn3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=4, bias=True)\n",
       "  (gn_relu): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CelebaNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "privacy_engine = PrivacyEngine(accountant=\"rdp\")\n",
    "\n",
    "(\n",
    "    private_model,\n",
    "    private_optimizer,\n",
    "    private_train_loader,\n",
    ") = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=5.0,\n",
    ")\n",
    "private_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcorbucci/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 \tLoss: 1.368281 Acc@1: 30.742811 (ε = 0.78, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.363325 Acc@1: 32.000202 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.356264 Acc@1: 35.127597 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.331021 Acc@1: 38.410917 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.312420 Acc@1: 41.351678 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.294038 Acc@1: 44.027400 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.272408 Acc@1: 46.442027 (ε = 0.83, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.244345 Acc@1: 48.729917 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.232480 Acc@1: 50.597119 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.218044 Acc@1: 52.069290 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.202789 Acc@1: 53.528384 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.184230 Acc@1: 54.902689 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.167134 Acc@1: 56.149107 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.152027 Acc@1: 57.228530 (ε = 0.84, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.140064 Acc@1: 58.273355 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.130935 Acc@1: 59.100832 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.122432 Acc@1: 59.914251 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.110437 Acc@1: 60.703819 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.101244 Acc@1: 61.404924 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.094660 Acc@1: 61.977432 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.083532 Acc@1: 62.605241 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.078454 Acc@1: 63.136041 (ε = 0.85, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.070804 Acc@1: 63.648561 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.063665 Acc@1: 64.146516 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.057060 Acc@1: 64.633239 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.054647 Acc@1: 65.048660 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.048645 Acc@1: 65.450396 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 0 \tLoss: 1.043469 Acc@1: 65.858163 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.915304 Acc@1: 76.108715 (ε = 0.86, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.909012 Acc@1: 76.452607 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.912350 Acc@1: 76.347435 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.920316 Acc@1: 76.147949 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.916340 Acc@1: 76.210162 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.910236 Acc@1: 76.183439 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.911042 Acc@1: 76.095576 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.907897 Acc@1: 76.193709 (ε = 0.87, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.904812 Acc@1: 76.283853 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.905252 Acc@1: 76.374122 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.905639 Acc@1: 76.403903 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.902248 Acc@1: 76.463641 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.901980 Acc@1: 76.490550 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.896585 Acc@1: 76.524954 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.895069 Acc@1: 76.563278 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.894791 Acc@1: 76.590225 (ε = 0.88, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.890180 Acc@1: 76.674794 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.886261 Acc@1: 76.734457 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.885816 Acc@1: 76.778465 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.885211 Acc@1: 76.822834 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.884783 Acc@1: 76.850840 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.881166 Acc@1: 76.908822 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.878905 Acc@1: 76.959402 (ε = 0.89, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.876046 Acc@1: 77.058252 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.874486 Acc@1: 77.078931 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.874137 Acc@1: 77.132023 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.871853 Acc@1: 77.180451 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 1 \tLoss: 0.871425 Acc@1: 77.232287 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.900959 Acc@1: 78.341558 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.860157 Acc@1: 78.348950 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.855873 Acc@1: 78.172346 (ε = 0.90, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.866071 Acc@1: 78.262311 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.869776 Acc@1: 78.179479 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.859453 Acc@1: 78.223098 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.858307 Acc@1: 78.270583 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.856155 Acc@1: 78.324294 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.857518 Acc@1: 78.442190 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.860208 Acc@1: 78.449781 (ε = 0.91, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.863074 Acc@1: 78.393351 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.863119 Acc@1: 78.324574 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.865227 Acc@1: 78.321577 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.862118 Acc@1: 78.335221 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.861367 Acc@1: 78.335934 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.861510 Acc@1: 78.322709 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.863984 Acc@1: 78.291467 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.868741 Acc@1: 78.282439 (ε = 0.92, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.873722 Acc@1: 78.226624 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.870733 Acc@1: 78.266373 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.869748 Acc@1: 78.324227 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.870242 Acc@1: 78.342102 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.872662 Acc@1: 78.320522 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.871782 Acc@1: 78.302547 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.874013 Acc@1: 78.302712 (ε = 0.93, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.875036 Acc@1: 78.329589 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.876510 Acc@1: 78.339232 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 2 \tLoss: 0.876493 Acc@1: 78.367351 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.887613 Acc@1: 78.874106 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.889064 Acc@1: 78.769953 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.892047 Acc@1: 78.914863 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.888150 Acc@1: 78.950254 (ε = 0.94, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.902656 Acc@1: 78.814861 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.908476 Acc@1: 78.637419 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.911901 Acc@1: 78.718168 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.914178 Acc@1: 78.601330 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.917747 Acc@1: 78.625667 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.916880 Acc@1: 78.677331 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.911596 Acc@1: 78.714025 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.912170 Acc@1: 78.706653 (ε = 0.95, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.908578 Acc@1: 78.735785 (ε = 0.96, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.909696 Acc@1: 78.763493 (ε = 0.96, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.907471 Acc@1: 78.813390 (ε = 0.96, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.904159 Acc@1: 78.872127 (ε = 0.96, δ = 1e-05)\n",
      "\tTrain Epoch: 3 \tLoss: 0.904403 Acc@1: 78.866400 (ε = 0.96, δ = 1e-05)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train(\n\u001b[1;32m      3\u001b[0m         model\u001b[39m=\u001b[39;49mprivate_model,\n\u001b[1;32m      4\u001b[0m         train_loader\u001b[39m=\u001b[39;49mprivate_train_loader,\n\u001b[1;32m      5\u001b[0m         optimizer\u001b[39m=\u001b[39;49mprivate_optimizer,\n\u001b[1;32m      6\u001b[0m         epoch\u001b[39m=\u001b[39;49miteration,\n\u001b[1;32m      7\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      8\u001b[0m         privacy_engine\u001b[39m=\u001b[39;49mprivacy_engine,\n\u001b[1;32m      9\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch, device, privacy_engine)\u001b[0m\n\u001b[1;32m     15\u001b[0m top1_acc \u001b[39m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m BatchMemoryManager(\n\u001b[1;32m     18\u001b[0m     data_loader\u001b[39m=\u001b[39mtrain_loader, max_physical_batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, optimizer\u001b[39m=\u001b[39moptimizer\n\u001b[1;32m     19\u001b[0m ) \u001b[39mas\u001b[39;00m memory_safe_data_loader:\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(memory_safe_data_loader):\n\u001b[1;32m     21\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mCelebaDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     57\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimages[index]\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m---> 60\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m     63\u001b[0m     img,\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets[index],\n\u001b[1;32m     65\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pistacchio-fl-simulator-XKOu9Fs5-py3.10/lib/python3.10/site-packages/PIL/Image.py:2174\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2167\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2168\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2169\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2170\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2171\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[0;32m-> 2174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for iteration in range(0, 10):\n",
    "    train(\n",
    "        model=private_model,\n",
    "        train_loader=private_train_loader,\n",
    "        optimizer=private_optimizer,\n",
    "        epoch=iteration,\n",
    "        device=device,\n",
    "        privacy_engine=privacy_engine,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pistacchio-fl-simulator-XKOu9Fs5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
